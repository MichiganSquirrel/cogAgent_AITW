{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbdc6052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install Dependencies\n",
    "# Run this once at the start\n",
    "!pip install -q torch transformers datasets accelerate pillow sentencepiece\n",
    "\n",
    "# If using 4-bit quantization (recommended for free Colab/T4 GPU)\n",
    "!pip install -q bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19131392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Configured.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Imports & Configuration\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "from typing import List, Dict, Tuple, Any, Optional\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm.notebook import tqdm  # Use notebook version of tqdm\n",
    "\n",
    "# Configuration\n",
    "# On Colab, we use the remote HuggingFace hub path directly\n",
    "MODEL_PATH = \"THUDM/cogagent-9b-20241220\" \n",
    "DATASET_NAME = \"google/android_in_the_wild\"\n",
    "SUBSET = \"general\" \n",
    "SPLIT = \"test\"\n",
    "MAX_SAMPLES = 10  # Start small to test the pipeline!\n",
    "\n",
    "# Constants\n",
    "ACTION_TYPE_MAP = {\n",
    "    0: \"touch\", 1: \"lift\", 2: \"type\", 3: \"scroll\",\n",
    "    4: \"press_back\", 5: \"press_home\", 6: \"press_enter\"\n",
    "}\n",
    "COGA_RES = 1000.0 \n",
    "DIST_THRESHOLD = 0.20 \n",
    "\n",
    "print(\"Environment Configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cddd63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Helper Functions & Visualization\n",
    "\n",
    "def calculate_centroid(box: List[int]) -> Tuple[float, float]:\n",
    "    if not box or len(box) != 4:\n",
    "        return (0.0, 0.0)\n",
    "    cx = (box[0] + box[2]) / 2.0\n",
    "    cy = (box[1] + box[3]) / 2.0\n",
    "    return (cx / COGA_RES, cy / COGA_RES)\n",
    "\n",
    "def euclidean_distance(p1: Tuple[float, float], p2: Tuple[float, float]) -> float:\n",
    "    return np.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)\n",
    "\n",
    "def parse_cogagent_output(text: str) -> Dict[str, Any]:\n",
    "    result = {\"action\": \"unknown\", \"point\": None, \"text\": None, \"raw_box\": None}\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Extract Box\n",
    "    box_match = re.search(r\"\\[\\[(\\d+),(\\d+),(\\d+),(\\d+)\\]\\]\", text)\n",
    "    if box_match:\n",
    "        coords = [int(c) for c in box_match.groups()]\n",
    "        result[\"raw_box\"] = coords\n",
    "        result[\"point\"] = calculate_centroid(coords)\n",
    "    \n",
    "    # Determine Action\n",
    "    lower_text = text.lower()\n",
    "    if \"click\" in lower_text or \"tap\" in lower_text:\n",
    "        result[\"action\"] = \"touch\"\n",
    "    elif \"type\" in lower_text:\n",
    "        result[\"action\"] = \"type\"\n",
    "        text_match = re.search(r\"(?:type|text)\\s*[:=]?\\s*['\\\"]([^'\\\"]+)['\\\"]\", lower_text)\n",
    "        if text_match: result[\"text\"] = text_match.group(1)\n",
    "    elif \"scroll\" in lower_text: result[\"action\"] = \"scroll\"\n",
    "    elif \"back\" in lower_text: result[\"action\"] = \"press_back\"\n",
    "    elif \"home\" in lower_text: result[\"action\"] = \"press_home\"\n",
    "    elif \"enter\" in lower_text: result[\"action\"] = \"press_enter\"\n",
    "        \n",
    "    return result\n",
    "\n",
    "def build_prompt(goal: str, history: List[str]) -> str:\n",
    "    history_str = \"History steps:\\n\" + (\"None\\n\" if not history else \"\".join([f\"{i}. {h}\\n\" for i, h in enumerate(history[-3:])]))\n",
    "    return f\"Task: {goal}\\n\\n{history_str}\\n(Platform: Android)\\n\\n(Answer in Action-Operation-Sensitive format with Grounded Operation.)\"\n",
    "\n",
    "def visualize_prediction(image, goal, pred, gt):\n",
    "    \"\"\"Draws GT (Green) and Pred (Red) on the image for debugging\"\"\"\n",
    "    img_viz = image.copy()\n",
    "    draw = ImageDraw.Draw(img_viz)\n",
    "    w, h = img_viz.size\n",
    "    \n",
    "    # Draw GT Point (Green Circle)\n",
    "    if gt['touch_point'] != (0.0, 0.0):\n",
    "        gt_x, gt_y = gt['touch_point'][0] * w, gt['touch_point'][1] * h\n",
    "        draw.ellipse((gt_x-10, gt_y-10, gt_x+10, gt_y+10), outline=\"green\", width=3)\n",
    "        \n",
    "    # Draw Pred Box (Red Rectangle)\n",
    "    if pred['raw_box']:\n",
    "        # CogAgent box is 0-1000, need to scale to image size\n",
    "        p_box = [c / 1000.0 for c in pred['raw_box']]\n",
    "        draw.rectangle(\n",
    "            [p_box[0]*w, p_box[1]*h, p_box[2]*w, p_box[3]*h], \n",
    "            outline=\"red\", width=3\n",
    "        )\n",
    "        \n",
    "    display(img_viz) # Jupyter/Colab function to show image\n",
    "    print(f\"Goal: {goal}\")\n",
    "    print(f\"GT: {gt['action_type']} | Pred: {pred['action']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7099cc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Helper Functions & Visualization\n",
    "\n",
    "def calculate_centroid(box: List[int]) -> Tuple[float, float]:\n",
    "    if not box or len(box) != 4:\n",
    "        return (0.0, 0.0)\n",
    "    cx = (box[0] + box[2]) / 2.0\n",
    "    cy = (box[1] + box[3]) / 2.0\n",
    "    return (cx / COGA_RES, cy / COGA_RES)\n",
    "\n",
    "def euclidean_distance(p1: Tuple[float, float], p2: Tuple[float, float]) -> float:\n",
    "    return np.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)\n",
    "\n",
    "def parse_cogagent_output(text: str) -> Dict[str, Any]:\n",
    "    result = {\"action\": \"unknown\", \"point\": None, \"text\": None, \"raw_box\": None}\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Extract Box\n",
    "    box_match = re.search(r\"\\[\\[(\\d+),(\\d+),(\\d+),(\\d+)\\]\\]\", text)\n",
    "    if box_match:\n",
    "        coords = [int(c) for c in box_match.groups()]\n",
    "        result[\"raw_box\"] = coords\n",
    "        result[\"point\"] = calculate_centroid(coords)\n",
    "    \n",
    "    # Determine Action\n",
    "    lower_text = text.lower()\n",
    "    if \"click\" in lower_text or \"tap\" in lower_text:\n",
    "        result[\"action\"] = \"touch\"\n",
    "    elif \"type\" in lower_text:\n",
    "        result[\"action\"] = \"type\"\n",
    "        text_match = re.search(r\"(?:type|text)\\s*[:=]?\\s*['\\\"]([^'\\\"]+)['\\\"]\", lower_text)\n",
    "        if text_match: result[\"text\"] = text_match.group(1)\n",
    "    elif \"scroll\" in lower_text: result[\"action\"] = \"scroll\"\n",
    "    elif \"back\" in lower_text: result[\"action\"] = \"press_back\"\n",
    "    elif \"home\" in lower_text: result[\"action\"] = \"press_home\"\n",
    "    elif \"enter\" in lower_text: result[\"action\"] = \"press_enter\"\n",
    "        \n",
    "    return result\n",
    "\n",
    "def build_prompt(goal: str, history: List[str]) -> str:\n",
    "    history_str = \"History steps:\\n\" + (\"None\\n\" if not history else \"\".join([f\"{i}. {h}\\n\" for i, h in enumerate(history[-3:])]))\n",
    "    return f\"Task: {goal}\\n\\n{history_str}\\n(Platform: Android)\\n\\n(Answer in Action-Operation-Sensitive format with Grounded Operation.)\"\n",
    "\n",
    "def visualize_prediction(image, goal, pred, gt):\n",
    "    \"\"\"Draws GT (Green) and Pred (Red) on the image for debugging\"\"\"\n",
    "    img_viz = image.copy()\n",
    "    draw = ImageDraw.Draw(img_viz)\n",
    "    w, h = img_viz.size\n",
    "    \n",
    "    # Draw GT Point (Green Circle)\n",
    "    if gt['touch_point'] != (0.0, 0.0):\n",
    "        gt_x, gt_y = gt['touch_point'][0] * w, gt['touch_point'][1] * h\n",
    "        draw.ellipse((gt_x-10, gt_y-10, gt_x+10, gt_y+10), outline=\"green\", width=3)\n",
    "        \n",
    "    # Draw Pred Box (Red Rectangle)\n",
    "    if pred['raw_box']:\n",
    "        # CogAgent box is 0-1000, need to scale to image size\n",
    "        p_box = [c / 1000.0 for c in pred['raw_box']]\n",
    "        draw.rectangle(\n",
    "            [p_box[0]*w, p_box[1]*h, p_box[2]*w, p_box[3]*h], \n",
    "            outline=\"red\", width=3\n",
    "        )\n",
    "        \n",
    "    display(img_viz) # Jupyter/Colab function to show image\n",
    "    print(f\"Goal: {goal}\")\n",
    "    print(f\"GT: {gt['action_type']} | Pred: {pred['action']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cc5a3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Loading CogAgent from THUDM/cogagent-9b-20241220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e84f4cfa5d48e69654c9bea6879417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bdcda0ee369449c8e225c2d32b993fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenization_chatglm.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/cogagent-9b-20241220:\n",
      "- tokenization_chatglm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce59651a607347908c0fb2c488a12577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/2.62M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec49504c09614df39e15743c76786459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc1da63ce362423da485211e87859e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_chatglm.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/cogagent-9b-20241220:\n",
      "- configuration_chatglm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ff8500e8224d779b4837775c5828ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_chatglm.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca31e2f8b8ac44649a45937c6cd64244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "visual.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/cogagent-9b-20241220:\n",
      "- visual.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/cogagent-9b-20241220:\n",
      "- modeling_chatglm.py\n",
      "- visual.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e75f4778a4446ac8ffb51e1f06b75fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad61e8ab1d8424aa39f1a3e4b384094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f143b50b5dbf41ddb9ef8fc8eec32711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00006.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5dbca3967774ea1b06de21ec512b8d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00006.safetensors:   0%|          | 0.00/3.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec31d869c8b1477da96219d702d0207b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00006.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b4a9a21b2847269be6ad0f25254023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00006.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ee87bd27374f17b88230441b849064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00006.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "100bb09771194a319cfb6701955c0098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00006.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39cdfde3a4824eda82646e9c3bc9b1c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f473ca48c2470d82a51e57c96bb7f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/211 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded!\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load Model\n",
    "print(f\"ü§ñ Loading CogAgent from {MODEL_PATH}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "\n",
    "# OPTION 1: High-RAM GPU (A100/L4)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ").eval()\n",
    "\n",
    "# OPTION 2: Low-RAM GPU (T4 - Free Colab) - Uncomment below if needed\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_PATH,\n",
    "#     trust_remote_code=True,\n",
    "#     load_in_4bit=True, \n",
    "#     device_map=\"auto\"\n",
    "# ).eval()\n",
    "\n",
    "print(\"Model Loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cbb09c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking GPU Status before loading...\n",
      "NVIDIA A100-SXM4-40GB\n",
      "Memory Allocated: 25.90 GB\n",
      "Memory Reserved:  25.91 GB\n",
      "ü§ñ Loading CogAgent from THUDM/cogagent-9b-20241220\n",
      "‚ö° Attempting to load model fully onto GPU (device_map='cuda')...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73e269d30794e9eb55f301ab14a0eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Load failed with device_map='cuda': CUDA out of memory. Tried to allocate 214.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 198.88 MiB is free. Process 29089 has 39.35 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 5.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "‚ö†Ô∏è Falling back to device_map='auto' (May be slow)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/accelerate/utils/modeling.py:1566: UserWarning: Current model requires 64 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5237ce0c9d7842eb86d8907eac030849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Applying compatibility patches...\n",
      "‚úÖ Patches applied.\n",
      "üìÇ Loading Dataset cjfcsjt/AITW_Single...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b4b3e8efe9543d0808b3ad142bb1fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb948bd8bed46e2b3f977bbdf1c38b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c90a863e95040bf82887eaf08b94996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Debug Evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4185c8dec99a4eb78a3157789a4debde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== SAMPLE 0 START =====\n",
      "[DEBUG] Goal: Go to amazon search bar\n",
      "    [DEBUG] Input Query: Task: Go to amazon search bar\n",
      "(Platform: Android)\n",
      "Answer in Action-Operation-Sensitive format with Grounded Operation.\n",
      "    [DEBUG] Image Size: (474, 1000)\n",
      "    [DEBUG] Image Tensor Shape: torch.Size([1, 3, 1120, 1120])\n",
      "    [DEBUG] Pre-processing took: 0.0576s\n",
      "    [DEBUG] Thread started, entering model.generate...\n",
      "    [DEBUG] Streaming tokens: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-411087023.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipython-input-411087023.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mstart_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcog_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgoal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"‚ùå Error: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-411087023.py\u001b[0m in \u001b[0;36mcog_generate\u001b[0;34m(model, tokenizer, img, query)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mDEBUG_MODE\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"    [DEBUG] Streaming tokens: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstreamer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mDEBUG_MODE\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{t}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mgenerated_text\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/streamers.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_signal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cell 5: Debug Mode Evaluation Loop (Deep Logging)\n",
    "import os, io, re, json, threading, torch, types, time, gc\n",
    "import numpy as np\n",
    "from typing import Any, List, Dict, Tuple\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel, TextIteratorStreamer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# ================= üîß Ë∞ÉËØïÂºÄÂÖ≥ =================\n",
    "DEBUG_MODE = True  # ÂºÄÂêØËØ¶ÁªÜÊó•Âøó\n",
    "\n",
    "# ================= Configuration =================\n",
    "MODEL_PATH = \"THUDM/cogagent-9b-20241220\"\n",
    "DATASET_NAME = \"cjfcsjt/AITW_Single\"\n",
    "SUBSET = \"unseen_subject\"\n",
    "TARGET_SPLIT = \"test\"\n",
    "LOG_FILE = \"/content/drive/MyDrive/cogagent_aitw_eval_log_debug.jsonl\" \n",
    "\n",
    "# ================= üöë COMPATIBILITY PATCH üöë =================\n",
    "def _fix_compatibility(model):\n",
    "    print(\"üîß Applying compatibility patches...\")\n",
    "    def _manual_extract_past(self, outputs, standardized_output_keys=None):\n",
    "        if hasattr(outputs, \"past_key_values\"): return \"past_key_values\", outputs.past_key_values\n",
    "        elif isinstance(outputs, (tuple, list)) and len(outputs) > 1: return \"past_key_values\", outputs[1]\n",
    "        return \"past_key_values\", None\n",
    "    \n",
    "    if not hasattr(model, \"_extract_past_from_model_output\"):\n",
    "        model._extract_past_from_model_output = types.MethodType(_manual_extract_past, model)\n",
    "    \n",
    "    try:\n",
    "        if not hasattr(model.config, 'num_hidden_layers'):\n",
    "            model.config.num_hidden_layers = model.config.num_layers\n",
    "    except AttributeError: pass\n",
    "    \n",
    "    model.config.use_cache = False\n",
    "    print(\"‚úÖ Patches applied.\")\n",
    "\n",
    "# ================= Helper Functions =================\n",
    "COGA_RES = 1000.0\n",
    "DIST_THRESHOLD = 0.20 \n",
    "ACTION_TYPE_MAP = {0: \"touch\", 1: \"lift\", 2: \"type\", 3: \"scroll\", 4: \"press_back\", 5: \"press_home\", 6: \"press_enter\"}\n",
    "\n",
    "def calculate_centroid(box: List[int]) -> Tuple[float, float]:\n",
    "    if not box or len(box) != 4: return (0.0, 0.0)\n",
    "    cx = (box[0] + box[2]) / 2.0\n",
    "    cy = (box[1] + box[3]) / 2.0\n",
    "    return (cx / COGA_RES, cy / COGA_RES)\n",
    "\n",
    "def euclidean_distance(p1: Tuple[float, float], p2: Tuple[float, float]) -> float:\n",
    "    return np.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)\n",
    "\n",
    "def build_prompt(goal: str) -> str:\n",
    "    return f\"Task: {goal}\\n(Platform: Android)\\nAnswer in Action-Operation-Sensitive format with Grounded Operation.\"\n",
    "\n",
    "def parse_output(text: str) -> Dict[str, Any]:\n",
    "    result = {\"action\": \"unknown\", \"point\": None, \"raw_box\": None}\n",
    "    text = text.strip().lower()\n",
    "    box_match = re.search(r\"\\[\\[(\\d+),(\\d+),(\\d+),(\\d+)\\]\\]\", text)\n",
    "    if box_match:\n",
    "        coords = [int(c) for c in box_match.groups()]\n",
    "        result[\"raw_box\"] = coords\n",
    "        result[\"point\"] = calculate_centroid(coords)\n",
    "    if \"click\" in text or \"tap\" in text: result[\"action\"] = \"touch\"\n",
    "    elif \"type\" in text: result[\"action\"] = \"type\"\n",
    "    elif \"scroll\" in text: result[\"action\"] = \"scroll\"\n",
    "    elif \"back\" in text: result[\"action\"] = \"press_back\"\n",
    "    elif \"home\" in text: result[\"action\"] = \"press_home\"\n",
    "    elif \"enter\" in text: result[\"action\"] = \"press_enter\"\n",
    "    return result\n",
    "\n",
    "# ================= Inference Engine (With Debug Prints) =================\n",
    "@torch.inference_mode()\n",
    "def cog_generate(model, tokenizer, img, query):\n",
    "    if DEBUG_MODE:\n",
    "        print(f\"    [DEBUG] Input Query: {query}\")\n",
    "        print(f\"    [DEBUG] Image Size: {img.size}\")\n",
    "    \n",
    "    # ÊûÑÈÄ†ËæìÂÖ•\n",
    "    start_time = time.time()\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"image\": img, \"content\": query}],\n",
    "        add_generation_prompt=True, tokenize=True, return_tensors=\"pt\", return_dict=True\n",
    "    ).to(model.device)\n",
    "    \n",
    "    if 'images' in inputs: \n",
    "        inputs['images'] = inputs['images'].to(torch.bfloat16)\n",
    "        if DEBUG_MODE: print(f\"    [DEBUG] Image Tensor Shape: {inputs['images'].shape}\")\n",
    "\n",
    "    inputs.pop(\"use_cache\", None)\n",
    "    inputs.pop(\"token_type_ids\", None)\n",
    "\n",
    "    if DEBUG_MODE: print(f\"    [DEBUG] Pre-processing took: {time.time() - start_time:.4f}s\")\n",
    "\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    \n",
    "    def run(): \n",
    "        if DEBUG_MODE: print(\"    [DEBUG] Thread started, entering model.generate...\")\n",
    "        try:\n",
    "            model.generate(**inputs, max_new_tokens=128, do_sample=False, streamer=streamer, use_cache=False)\n",
    "        except Exception as e:\n",
    "            print(f\"    [DEBUG] ‚ùå Generation Thread Error: {e}\")\n",
    "\n",
    "    thread = threading.Thread(target=run)\n",
    "    thread.start()\n",
    "    \n",
    "    generated_text = \"\"\n",
    "    if DEBUG_MODE: print(\"    [DEBUG] Streaming tokens: \", end=\"\")\n",
    "    \n",
    "    for t in streamer:\n",
    "        if DEBUG_MODE: print(f\"{t}\", end=\"\", flush=True)\n",
    "        generated_text += t\n",
    "        \n",
    "    if DEBUG_MODE: print(\"\\n    [DEBUG] Generation finished.\")\n",
    "    thread.join()\n",
    "    \n",
    "    del inputs\n",
    "    return generated_text.strip()\n",
    "\n",
    "# ================= MAIN EXECUTION =================\n",
    "def main():\n",
    "    # 1. ÊòæÂ≠òÊ£ÄÊü•\n",
    "    print(\"üîç Checking GPU Status before loading...\")\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(f\"Memory Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"Memory Reserved:  {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # 2. Âä†ËΩΩÊ®°Âûã (Â∞ùËØïÂº∫Âà∂Âä†ËΩΩÂà∞ CUDA)\n",
    "    print(f\"ü§ñ Loading CogAgent from {MODEL_PATH}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "    \n",
    "    try:\n",
    "        # ‚ö†Ô∏è Âº∫Âà∂‰ΩøÁî® cuda:0ÔºåÂ¶ÇÊûúÊòæÂ≠ò‰∏çÂ§üÁõ¥Êé•ÁÇ∏Âá∫Êù•ÔºåËÄå‰∏çÊòØÂÅ∑ÂÅ∑offload\n",
    "        print(\"‚ö° Attempting to load model fully onto GPU (device_map='cuda')...\")\n",
    "        model = AutoModel.from_pretrained(\n",
    "            MODEL_PATH, \n",
    "            trust_remote_code=True, \n",
    "            torch_dtype=torch.bfloat16, \n",
    "            device_map=\"cuda\" \n",
    "        ).eval()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Load failed with device_map='cuda': {e}\")\n",
    "        print(\"‚ö†Ô∏è Falling back to device_map='auto' (May be slow)...\")\n",
    "        model = AutoModel.from_pretrained(\n",
    "            MODEL_PATH, \n",
    "            trust_remote_code=True, \n",
    "            torch_dtype=torch.bfloat16, \n",
    "            device_map=\"auto\" \n",
    "        ).eval()\n",
    "\n",
    "    _fix_compatibility(model)\n",
    "\n",
    "    # 3. Êï∞ÊçÆÈõÜ\n",
    "    print(f\"üìÇ Loading Dataset {DATASET_NAME}...\")\n",
    "    try: ds = load_dataset(DATASET_NAME, SUBSET, split=TARGET_SPLIT, streaming=True)\n",
    "    except: ds = load_dataset(DATASET_NAME, SUBSET, split=\"train\", streaming=True)\n",
    "\n",
    "    print(\"üöÄ Starting Debug Evaluation...\")\n",
    "    \n",
    "    for i, sample in tqdm(enumerate(ds)):\n",
    "        if i >= 5: # Debug Ê®°ÂºèÂè™Ë∑ë 5 Êù°\n",
    "            print(\"üõë Debug limit reached (5 samples).\")\n",
    "            break\n",
    "\n",
    "        print(f\"\\n===== SAMPLE {i} START =====\")\n",
    "        \n",
    "        # --- Data Prep ---\n",
    "        raw_img = sample.get(\"image_encoded\")\n",
    "        if raw_img is None: continue\n",
    "        try: \n",
    "            img = raw_img if isinstance(raw_img, Image.Image) else Image.open(raw_img).convert(\"RGB\")\n",
    "            # Áº©Â∞è‰∏ÄÁÇπ‰ª•Âä†Âø´ debug ÈÄüÂ∫¶\n",
    "            if max(img.size) > 1000: img.thumbnail((1000, 1000))\n",
    "        except: continue\n",
    "        \n",
    "        goal = sample.get(\"goal_info\", \"\")\n",
    "        print(f\"[DEBUG] Goal: {goal}\")\n",
    "\n",
    "        # --- Inference ---\n",
    "        start_t = time.time()\n",
    "        try: \n",
    "            response = cog_generate(model, tokenizer, img, build_prompt(goal))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            continue\n",
    "        end_t = time.time()\n",
    "        \n",
    "        print(f\"[DEBUG] Full Inference Time: {end_t - start_t:.2f}s\")\n",
    "        print(f\"[DEBUG] Final Response: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f62b7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
